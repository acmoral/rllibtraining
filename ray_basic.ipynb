{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 10:51:46,197\tINFO worker.py:1555 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.7.15</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.7.15', ray_version='3.0.0.dev0', ray_commit='d124fbe71f0a5052eee571e27fe13f7ff99f5427', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:63788', 'raylet_socket_name': 'tcp://127.0.0.1:63887', 'webui_url': '', 'session_dir': 'C:\\\\Users\\\\acmor\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2023-01-28_10-51-43_877551_7800', 'metrics_export_port': 56984, 'gcs_address': '127.0.0.1:57777', 'address': '127.0.0.1:57777', 'dashboard_agent_listen_port': 52365, 'node_id': 'c7e7a4986824a85871757236061f1dd247e053b88a59739fa8c1ccd1'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See details of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'NodeID': '6af46194a6a491969ac45340d9f8dd32a43699de37cbca23d51ed560',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '127.0.0.1',\n",
       "  'NodeManagerHostname': 'cloudy',\n",
       "  'NodeManagerPort': 52189,\n",
       "  'ObjectManagerPort': 52187,\n",
       "  'ObjectStoreSocketName': 'tcp://127.0.0.1:52636',\n",
       "  'RayletSocketName': 'tcp://127.0.0.1:53222',\n",
       "  'MetricsExportPort': 55843,\n",
       "  'NodeName': '127.0.0.1',\n",
       "  'alive': True,\n",
       "  'Resources': {'object_store_memory': 379942502.0,\n",
       "   'node:127.0.0.1': 1.0,\n",
       "   'memory': 759885006.0,\n",
       "   'CPU': 8.0}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2023-01-26 21:30:52,045 C 17320 15044] (raylet.exe) util.cc:59:  Check failed: e.size() == ep.size() \n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m unknown\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m BaseThreadInitThunk\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m RtlUserThreadStart\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m \n",
      "2023-01-26 21:31:06,295\tWARNING worker.py:1868 -- The node with node id: 6af46194a6a491969ac45340d9f8dd32a43699de37cbca23d51ed560 and address: 127.0.0.1 and node name: 127.0.0.1 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, preempted node, etc.) \n",
      "\t(2) raylet has lagging heartbeats due to slow network or busy workload.\n"
     ]
    }
   ],
   "source": [
    "ray.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:2.8 seconds,data:\n",
      "(0, 'Learning')\n",
      "(1, 'Ray')\n",
      "(2, 'Flexible')\n",
      "(3, 'Distributed')\n",
      "(4, 'Python')\n",
      "(5, 'for')\n",
      "(6, 'Data')\n",
      "(7, 'Science')\n"
     ]
    }
   ],
   "source": [
    "#example of Ray API\n",
    "import time\n",
    "database = [\"Learning\", \"Ray\",\n",
    " \"Flexible\", \"Distributed\", \"Python\", \"for\", \"Data\", \"Science\"\n",
    "]\n",
    "def retrieve(item):\n",
    "    time.sleep(item/10.)\n",
    "    return item,database[item]\n",
    "def print_runtime(input_data,start_time,decimals=1):\n",
    "    print(f'Runtime:{time.time()-start_time:.{decimals}f} seconds,data:')\n",
    "    print(*input_data,sep=\"\\n\")\n",
    "start = time.time()\n",
    "data = [retrieve(item) for item in range(8)]\n",
    "print_runtime(data,start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def retrieve_task(item):\n",
    "    return retrieve(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:0.81 seconds,data:\n",
      "(0, 'Learning')\n",
      "(1, 'Ray')\n",
      "(2, 'Flexible')\n",
      "(3, 'Distributed')\n",
      "(4, 'Python')\n",
      "(5, 'for')\n",
      "(6, 'Data')\n",
      "(7, 'Science')\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data_references = [retrieve_task.remote(item) for item in range(8)]\n",
    "data  = ray.get(data_references)\n",
    "print_runtime(data,start,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_object_ref = ray.put(database)\n",
    "@ray.remote\n",
    "def retrieve_task(item):\n",
    " obj_store_data = ray.get(database_object_ref) \n",
    " time.sleep(item / 10.)\n",
    " return item, obj_store_data[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:0.77 seconds,data:\n",
      "(0, 'Learning')\n",
      "(1, 'Ray')\n",
      "(2, 'Flexible')\n",
      "(3, 'Distributed')\n",
      "(4, 'Python')\n",
      "(5, 'for')\n",
      "(6, 'Data')\n",
      "(7, 'Science')\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data_references = [retrieve_task.remote(item) for item in range(8)]\n",
    "data  = ray.get(data_references)\n",
    "print_runtime(data,start,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building a RL maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class Discrete:\n",
    "    def __init__(self,num_actions:int):\n",
    "        \"\"\"Discrete action space for num_actions.\n",
    "            Discreete(4) can be used as encoding moving in one of the \n",
    "            cardinal directions\"\"\"\n",
    "        self.n=  num_actions\n",
    "    def sample(self):\n",
    "        return random.randint(0,self.n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "space = Discrete(4)\n",
    "print(space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "class Environment:\n",
    "    seeker,goal = (0,0), (4,4)\n",
    "    info = {'seeker':seeker,'goal':goal}\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "    def reset(self):\n",
    "        \"\"\"reset seekers and goals position, return obs\"\"\"\n",
    "        self.seeker = (0,0)\n",
    "        self.goal = (4,4)\n",
    "        return self.get_observation()\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return 5*self.seeker[0] +self.seeker[1]\n",
    "    def get_reward(self):\n",
    "        \"\"\"reward finding the goal\"\"\"\n",
    "        return 1 if self.seeker == self.goal else 0\n",
    "    def is_done(self):\n",
    "        \"\"\"We're done if we found the goal\"\"\"\n",
    "        return self.seeker == self.goal\n",
    "    def step(self,action):\n",
    "        \"\"\"Take a step in a direction and return all available\n",
    "        information\"\"\"\n",
    "        if action == 0: #move down\n",
    "            self.seeker = (min(self.seeker[0]+1,4),self.seeker[1])\n",
    "        elif action == 1: # move left\n",
    "            self.seeker = (self.seeker[0],max(self.seeker[1]-1,0))\n",
    "        elif action == 2: # move up\n",
    "            self.seeker = (max(self.seeker[0]-1,0),self.seeker[1])\n",
    "        elif action == 3: # move right\n",
    "            self.seeker = (self.seeker[0],min(self.seeker[1]+1,4))\n",
    "        else:\n",
    "            raise ValueError(\"invalid action\")\n",
    "        return self.get_observation(),self.get_reward(),self.is_done(),self.info\n",
    "    def render(self, *args,**kwargs):\n",
    "        \"\"\"Render the environment, e.g by printing its representation\"\"\"\n",
    "        clear_output()\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S' \n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "environment = Environment()\n",
    "\n",
    "while not environment.is_done():\n",
    "    random_action = environment.action_space.sample()\n",
    "    environment.step(random_action)\n",
    "    time.sleep(0.8)\n",
    "    environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Policy:\n",
    "    def __init__(self,env):\n",
    "        \"\"\"A policy suggest actions based on the current state.\n",
    "        We do this by tracking the value of each state-action pair\"\"\"\n",
    "        self.state_action_table = [\n",
    "            [ 0 for _ in range(env.action_space.n)] for _ in range(env.observation_space.n)\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "    def get_action(self,state,explore=True,epsilon=0.1):\n",
    "        \"\"\"Explore randomly or exploit the best value currently available\"\"\"\n",
    "        if explore and random.uniform(0,1) < epsilon:\n",
    "            #explore 10% of the times\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation(object):\n",
    "    def __init__(self,env):\n",
    "        \"\"\"simulates rollouts of an environment. given a policy\n",
    "        to follow\"\"\"\n",
    "        self.env = env\n",
    "    def rollouts(self,policy,render=False,explore=True,epsilon=0.1):\n",
    "        \"\"\"Returns experiences for a policy rollout\"\"\"\n",
    "        experiences = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get_action(state,explore,epsilon)\n",
    "            next_state ,reward, done, info = self.env.step(action)\n",
    "            experiences.append([state,action,reward,next_state])\n",
    "            state = next_state\n",
    "            if render:\n",
    "                time.sleep(0.8)\n",
    "                self.env.render()\n",
    "        return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_policy = Policy(environment)\n",
    "sim = Simulation(environment)\n",
    "exp = sim.rollouts(untrained_policy,render = True, epsilon =1.0)\n",
    "for row in untrained_policy.state_action_table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy( policy,experiences,weight=0.1,discount_factor=0.9):\n",
    "    \"\"\"Updates a given policy with a list of (state, action, reward, state)\n",
    "    experiences.\"\"\"\n",
    "    for state,action,reward,next_state in experiences:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action]\n",
    "        new_value = (1-weight)*value+weight*(reward+discount_factor*next_max)\n",
    "        policy.state_action_table[state][action] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(env,num_episodes=10000,weight=0.1,discount_factor=0.9):\n",
    "    \"\"\"Training  policy by updating it with rollout experiences\"\"\"\n",
    "    policy = Policy(env)\n",
    "    sim =  Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollouts(policy)\n",
    "        update_policy(policy,experiences,weight,discount_factor)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_policy = train_policy(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n",
      "8.0 steps on averagefor a total of 10 episodes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained policy through rollouts\"\"\"\n",
    "    simulation =  Simulation(env)\n",
    "    steps = 0\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollouts(policy,render=True,explore=False)\n",
    "        steps+=len(experiences)\n",
    "    print(f\"{steps/num_episodes} steps on average\"\n",
    "        f\"for a total of {num_episodes} episodes\")\n",
    "    return steps/num_episodes\n",
    "evaluate_policy(environment,trained_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 14:34:39,186\tINFO worker.py:1555 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()\n",
    "environment = Environment()\n",
    "env_ref = ray.put(environment)\n",
    "@ray.remote\n",
    "def create_policy():\n",
    "    env = ray.get(env_ref)\n",
    "    return Policy(env)\n",
    "@ray.remote \n",
    "class SimulationActor(Simulation):\n",
    "    def __init__(self):\n",
    "        env = ray.get(env_ref)\n",
    "        super().__init__(env)\n",
    "@ray.remote\n",
    "def update_policy_task(policy_ref, experiences_list):\n",
    "    \"\"\"Remote Ray task for updating a policy with experiences in parallel.\"\"\"\n",
    "    [update_policy(policy_ref, ray.get(xp)) for xp in experiences_list] \n",
    "    return policy_ref\n",
    "def train_policy_parallel(num_episodes=1000, num_simulations=4):\n",
    "    \"\"\"Parallel policy training function.\"\"\"\n",
    "    policy = create_policy.remote() \n",
    "    simulations = [SimulationActor.remote() for _ in range(num_simulations)] \n",
    "    for _ in range(num_episodes):\n",
    "        experiences = [sim.rollouts.remote(policy) for sim in simulations] \n",
    "        policy = update_policy_task.remote(policy, experiences) \n",
    "    return ray.get(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |S|\n",
      "\n",
      "8.0 steps on averagefor a total of 10 episodes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_policy = train_policy_parallel()\n",
    "evaluate_policy(environment,parallel_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5675d785b30bf8b4035d89e68a42c147013ad89d38be2d93ef0d6273a4f5a998"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
