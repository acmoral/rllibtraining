{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import time\n",
    "from gymnasium.wrappers import EnvCompatibility\n",
    "from IPython.display import clear_output\n",
    "class VacuumCleaner(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        x = np.array([3,3,3,3],dtype=int)\n",
    "        self.observation_space = MultiDiscrete(x)\n",
    "        self.action_space = Discrete(5)\n",
    "        self.max_episode_steps = 100\n",
    "    def reset(self,seed=None,options=None):\n",
    "        self.player = (1, 1) # the player starts at the top-left\n",
    "        self.suck = True\n",
    "        self.count=0\n",
    "        self.steps =0\n",
    "        self.field = np.array([\n",
    "            [1,1,1,1,1,1],\n",
    "            [1,0,2,0,0,1], # FFFF \n",
    "            [1,0,1,0,1,1], # FHFH\n",
    "            [1,0,0,0,1,1], # FFFH\n",
    "            [1,1,2,0,0,1],\n",
    "            [1,1,1,1,1,1]  # HFFF\n",
    "        ])\n",
    "        obs = self.observation()\n",
    "        info={}\n",
    "        return obs,info\n",
    "    def info(self):\n",
    "        return []\n",
    "    def observation(self):\n",
    "        obs = [self.field[self.player[0]+1,self.player[1]],\n",
    "                self.field[self.player[0],self.player[1]+1],\n",
    "                self.field[self.player[0]-1,self.player[1]],\n",
    "                self.field[self.player[0],self.player[1]-1]]\n",
    "        obs = np.array(obs, dtype=int)\n",
    "        return obs\n",
    "                \n",
    "    def reward(self):\n",
    "        if self.suck:\n",
    "            if self.field[self.player] == 2:\n",
    "                r = 2\n",
    "                self.field[self.player] = 0\n",
    "                self.count +=1\n",
    "            else:\n",
    "                r=-0.2\n",
    "        if self.count == 2 and self.player ==(0,0):\n",
    "            r = 3\n",
    "        else:\n",
    "            r=-0.1\n",
    "        return r  \n",
    "    def done(self):\n",
    "        return self.count == 2 and self.player == (0,0)\n",
    "\n",
    "    def is_valid_loc(self, location):\n",
    "        if self.field[location] == 1:\n",
    "                return False\n",
    "        else:\n",
    "                return True\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        self.steps+=1\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        elif action == 4:\n",
    "            if self.suck:\n",
    "                self.suck = False\n",
    "            else:\n",
    "                self.suck = True\n",
    "            new_loc = self.player\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3,4}\")\n",
    "\n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        if self.steps==self.max_episode_steps:\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "        info = {}\n",
    "        return (self.observation(),self.reward(),self.done(),truncated,info)\n",
    "\n",
    "    def render(self):\n",
    "        clear_output()\n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                if (i,j) == self.player:\n",
    "                    if self.suck:\n",
    "                        print(\"ðŸ‘¾))\", end=\"\")\n",
    "                    else:\n",
    "                        print(\" ðŸ‘¾ \", end=\"\")\n",
    "                elif self.field[(i,j)] == 2:\n",
    "                    print(\" ðŸ¦  \", end=\"\")\n",
    "                elif self.field[(i,j)]==1:\n",
    "                    print(\" ðŸ§± \", end=\"\")\n",
    "                else:\n",
    "                    print(\" ðŸ”² \", end=\"\")\n",
    "            print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ray' has no attribute 'rllib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1524\\413629312.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVacuumCleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\acmor\\miniconda3\\envs\\RL\\lib\\site-packages\\ray\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"workflow\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"module {__name__!r} has no attribute {name!r}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ray' has no attribute 'rllib'"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.rllib.utils.check_env(VacuumCleaner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "def env_creator(env_config):\n",
    "    # wrap and return an instance of your custom class\n",
    "    return EnvCompatibility(VacuumCleaner())\n",
    "register_env('VacuumCleaner', env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VacuumCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§± [1 0 0 1]\n",
      "\n",
      " ðŸ§±  ðŸ”²  ðŸ¦   ðŸ”²  ðŸ”²  ðŸ§± [1 0 0 1]\n",
      "\n",
      " ðŸ§±  ðŸ”²  ðŸ§±  ðŸ”²  ðŸ§±  ðŸ§± [1 0 0 1]\n",
      "\n",
      " ðŸ§±  ðŸ”²  ðŸ”²  ðŸ”²  ðŸ§±  ðŸ§± [1 0 0 1]\n",
      "\n",
      " ðŸ§±  ðŸ§± ðŸ‘¾)) ðŸ”²  ðŸ”²  ðŸ§± [1 0 0 1]\n",
      "\n",
      " ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§± [1 0 0 1]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Action must be in {0,1,2,3,4}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10076\\3920476044.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10076\\1261825499.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mnew_loc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Action must be in {0,1,2,3,4}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# Update the player location only if you stayed in bounds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Action must be in {0,1,2,3,4}"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "while True:\n",
    "    env.step(int(input()))\n",
    "    env.reward()\n",
    "    env.render()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,227\tWARNING env.py:157 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,228\tWARNING env.py:167 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,248\tINFO policy.py:1198 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,250\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15408)\u001b[0m 2023-01-29 12:27:20,248\tINFO policy.py:1198 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15408)\u001b[0m 2023-01-29 12:27:20,250\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12028)\u001b[0m 2023-01-29 12:27:20,250\tINFO policy.py:1198 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12028)\u001b[0m 2023-01-29 12:27:20,250\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15772)\u001b[0m 2023-01-29 12:27:20,250\tINFO policy.py:1198 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15772)\u001b[0m 2023-01-29 12:27:20,250\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "2023-01-29 12:27:20,285\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (MultiDiscrete([3 3 3 3]), Discrete(5)), '__env__': (MultiDiscrete([3 3 3 3]), Discrete(5))}\n",
      "2023-01-29 12:27:20,290\tINFO policy.py:1198 -- Policy (worker=local) running on CPU.\n",
      "2023-01-29 12:27:20,290\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "2023-01-29 12:27:20,298\tINFO rollout_worker.py:2040 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2023-01-29 12:27:20,306\tINFO rollout_worker.py:2041 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.OneHotPreprocessor object at 0x000002B5E4230488>}\n",
      "2023-01-29 12:27:20,306\tINFO rollout_worker.py:757 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x000002B5AB8B5288>})\n",
      "2023-01-29 12:27:20,314\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,325\tINFO rollout_worker.py:907 -- Generating sample batch of size 1000\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,325\tINFO sampler.py:609 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=int32, min=0.0, max=2.0, mean=1.0)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,325\tINFO sampler.py:610 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,325\tINFO sampler.py:852 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.333)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,325\tINFO sampler.py:857 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.333)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,331\tINFO sampler.py:1143 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                                   'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                                   'prev_action': None,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                                   'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                         'type': '_PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,331\tINFO sampler.py:1171 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 5), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-1.603, max=-1.603, mean=-1.603),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.201, max=0.201, mean=0.201),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.002, max=-0.002, mean=-0.002)})}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,331\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements t.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements agent_index.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements actions.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m -0.1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements rewards.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m False\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements terminateds.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m False\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements truncateds.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m -0.0017494894564151764\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements vf_preds.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m -1.6027793884277344\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements action_logp.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,339\tWARNING agent_collector.py:177 -- Provided tensor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m  does not match space of view requirements infos.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m Provided tensor has shape () and view requirement has shape shape ().Make sure dimensions match to resolve this warning.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:20,825\tINFO simple_list_collector.py:520 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((100, 5), dtype=float32, min=-0.005, max=0.008, mean=0.002),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'action_logp': np.ndarray((100,), dtype=float32, min=-1.615, max=-1.603, mean=-1.609),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'actions': np.ndarray((100,), dtype=int64, min=0.0, max=4.0, mean=2.13),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'advantages': np.ndarray((100,), dtype=float32, min=-6.338, max=-0.107, mean=-3.731),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'agent_index': np.ndarray((100,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'eps_id': np.ndarray((100,), dtype=int64, min=4.65341382696665e+17, max=4.65341382696665e+17, mean=4.65341382696665e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'infos': np.ndarray((100,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'new_obs': np.ndarray((100, 12), dtype=float32, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'obs': np.ndarray((100, 12), dtype=float32, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'rewards': np.ndarray((100,), dtype=float32, min=-0.1, max=-0.1, mean=-0.1),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               't': np.ndarray((100,), dtype=int32, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'terminateds': np.ndarray((100,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'truncateds': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'unroll_id': np.ndarray((100,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'value_targets': np.ndarray((100,), dtype=float32, min=-6.34, max=-0.1, mean=-3.724),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m               'vf_preds': np.ndarray((100,), dtype=float32, min=-0.002, max=0.01, mean=0.007)}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m 2023-01-29 12:27:22,980\tINFO rollout_worker.py:946 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m { 'action_dist_inputs': np.ndarray((1000, 5), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'action_logp': np.ndarray((1000,), dtype=float32, min=-1.616, max=-1.603, mean=-1.609),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'actions': np.ndarray((1000,), dtype=int64, min=0.0, max=4.0, mean=1.999),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'advantages': np.ndarray((1000,), dtype=float32, min=-6.338, max=-0.099, mean=-3.724),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'agent_index': np.ndarray((1000,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'eps_id': np.ndarray((1000,), dtype=int64, min=2.7893403809154736e+16, max=9.190937925565105e+17, mean=4.170971250695634e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'new_obs': np.ndarray((1000, 12), dtype=float32, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'obs': np.ndarray((1000, 12), dtype=float32, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'rewards': np.ndarray((1000,), dtype=float32, min=-0.1, max=-0.1, mean=-0.1),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   't': np.ndarray((1000,), dtype=int32, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'terminateds': np.ndarray((1000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'truncateds': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'unroll_id': np.ndarray((1000,), dtype=int32, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'value_targets': np.ndarray((1000,), dtype=float32, min=-6.34, max=-0.09, mean=-3.722),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m   'vf_preds': np.ndarray((1000,), dtype=float32, min=-0.002, max=0.01, mean=0.003)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18524)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO,PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "algo =  (PPOConfig()\n",
    "        .resources(num_gpus=0) # i dont have gpu \n",
    "        .rollouts(num_rollout_workers=4) # with low memory, best use just one worker\n",
    "        .environment(env=VacuumCleaner) # ray has built in gymnasium envs\n",
    "        .framework(\"torch\") # tensorflow also available, but i installed torch so im using this one\n",
    "        .build() # not sure what this does , yet\n",
    ")\n",
    "\n",
    "mean_ = []\n",
    "min_= []\n",
    "max_= []\n",
    "episodes = []\n",
    "\n",
    "for i in range(50):\n",
    "    result = algo.train()\n",
    "    mean_.append(result['episode_reward_mean'])\n",
    "    min_.append(result['episode_reward_min'])\n",
    "    max_.append(result['episode_reward_max'])\n",
    "    episodes.append(result['episodes_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = algo.save('./vacuum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§± \n",
      " ðŸ§±  ðŸ”²  ðŸ¦   ðŸ”²  ðŸ”²  ðŸ§± \n",
      " ðŸ§±  ðŸ”²  ðŸ§±  ðŸ”²  ðŸ§±  ðŸ§± \n",
      " ðŸ§±  ðŸ”²  ðŸ”²  ðŸ”²  ðŸ§±  ðŸ§± \n",
      " ðŸ§±  ðŸ§±  ðŸ”²  ðŸ”² ðŸ‘¾)) ðŸ§± \n",
      " ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§±  ðŸ§± \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1524\\2720527319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_single_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = VacuumCleaner()\n",
    "rewards =[]\n",
    "for i in range(10):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    while not terminated and not truncated:\n",
    "        env.render()\n",
    "        time.sleep(1)\n",
    "        action = algo.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5675d785b30bf8b4035d89e68a42c147013ad89d38be2d93ef0d6273a4f5a998"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
